{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a778cVvmAk-r",
        "outputId": "4b190ca8-7d6f-44d7-bf9e-9ccd0cf41b03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- BAB 4: MELATIH MODEL ---\n",
            "Semua library telah diimpor.\n",
            "\n",
            "[1. Regresi Linear]\n",
            "  Melatih dengan Persamaan Normal...\n",
            "  Theta terbaik (dari Persamaan Normal): \n",
            "[[3.86501051]\n",
            " [3.13916179]]\n",
            "  Theta terbaik (dari Scikit-Learn): \n",
            "  Intercept: [3.86501051], Coef: [[3.13916179]]\n",
            "  Melatih dengan Gradient Descent...\n",
            "  Theta terbaik (dari Batch GD): \n",
            "[[3.86501051]\n",
            " [3.13916179]]\n",
            "  Melatih dengan Stochastic GD (SGD)...\n",
            "  Theta terbaik (dari SGD): \n",
            "[[3.91897165]\n",
            " [3.13680279]]\n",
            "  Theta terbaik (dari SGDRegressor SK-Learn): \n",
            "  Intercept: [3.86256592], Coef: [3.15101583]\n",
            "\n",
            "[2. Regresi Polinomial]\n",
            "  Theta (polinomial): Intercept: [1.69877159], Coef: [[1.0346329  0.54852217]]\n",
            "\n",
            "[3. Kurva Belajar (Learning Curves)]\n",
            "\n",
            "[4. Model Linear yang Diregularisasi]\n",
            "  Implementasi Early Stopping (konseptual)...\n",
            "\n",
            "[5. Regresi Logistik dan Softmax]\n",
            "  Prediksi Softmax (multikelas) untuk [5, 2]: [2]\n",
            "  Probabilitas Softmax (multikelas) untuk [5, 2]: \n",
            "[[6.21626374e-07 5.73689802e-02 9.42630398e-01]]\n",
            "\n",
            "--- Selesai Bab 4 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# MENGIMPOR SEMUA LIBRARY YANG DIPERLUKAN UNTUK BAB 4\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.base import clone # Diperlukan untuk early stopping\n",
        "\n",
        "# Mengatur default plotting\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Untuk plot di dalam notebook (jika Anda menggunakan Jupyter)\n",
        "# %matplotlib inline\n",
        "\n",
        "print(\"--- BAB 4: MELATIH MODEL ---\")\n",
        "print(\"Semua library telah diimpor.\")\n",
        "\n",
        "# 1. REGRESI LINEAR\n",
        "# Mari kita fokus pada cara melatih model linear.\n",
        "# Modelnya: y_hat = theta_0 + theta_1*x_1 + ... + theta_n*x_n\n",
        "# Cost Function (Fungsi Biaya): Mean Squared Error (MSE)\n",
        "print(\"\\n[1. Regresi Linear]\")\n",
        "\n",
        "# Membuat data linear palsu untuk pengujian\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1) # y = 4 + 3x + noise\n",
        "\n",
        "# 1a. Metode 1: The Normal Equation (Persamaan Normal)\n",
        "print(\"  Melatih dengan Persamaan Normal...\")\n",
        "# Teori: Persamaan Normal adalah solusi analitik (rumus pasti)\n",
        "# untuk menemukan nilai theta yang meminimalkan cost function.\n",
        "# Rumus: theta_best = (X_transpose * X)^-1 * X_transpose * y\n",
        "# Kita perlu menambahkan x0 = 1 (bias term) ke setiap instance.\n",
        "X_b = np.c_[np.ones((100, 1)), X]  # np.c_ adalah concatenate\n",
        "theta_best_normal = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
        "\n",
        "print(f\"  Theta terbaik (dari Persamaan Normal): \\n{theta_best_normal}\")\n",
        "# Hasilnya harusnya dekat dengan [4] dan [3]\n",
        "\n",
        "# Mari kita prediksi menggunakan theta ini\n",
        "# X_new = np.array([[0], [2]])\n",
        "# X_new_b = np.c_[np.ones((2, 1)), X_new] # Tambahkan x0 = 1\n",
        "# y_predict = X_new_b.dot(theta_best_normal)\n",
        "# print(f\"  Prediksi: \\n{y_predict}\")\n",
        "\n",
        "# Plot hasil Persamaan Normal (di-comment)\n",
        "# plt.plot(X_new, y_predict, \"r-\")\n",
        "# plt.plot(X, y, \"b.\")\n",
        "# plt.axis([0, 2, 0, 15])\n",
        "# plt.title(\"Regresi Linear via Persamaan Normal\")\n",
        "# plt.show()\n",
        "\n",
        "# 1b. Menggunakan Scikit-Learn LinearRegression\n",
        "# Scikit-Learn melakukan hal yang sama (menggunakan SVD, bukan inversi matriks)\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)\n",
        "print(f\"  Theta terbaik (dari Scikit-Learn): \\n  Intercept: {lin_reg.intercept_}, Coef: {lin_reg.coef_}\")\n",
        "\n",
        "# 1c. Metode 2: Gradient Descent (GD)\n",
        "print(\"  Melatih dengan Gradient Descent...\")\n",
        "# Teori: GD adalah algoritma optimasi iteratif.\n",
        "# Ia menyesuaikan parameter (theta) sedikit demi sedikit untuk\n",
        "# menemukan nilai minimum dari Cost Function (MSE).\n",
        "# Hyperparameter kuncinya adalah 'learning_rate' (eta).\n",
        "\n",
        "# PENTING: GD SANGAT SENSITIF PADA SKALA FITUR.\n",
        "# Kita harus melakukan feature scaling (misal StandardScaler)\n",
        "# sebelum menggunakan GD. Namun, karena data X kita skalanya sudah\n",
        "# kecil (0-2), kita bisa melewatinya untuk contoh sederhana ini.\n",
        "\n",
        "# Varian 1: Batch Gradient Descent (BGD)\n",
        "# BGD menggunakan *seluruh* dataset latih pada setiap langkah.\n",
        "eta = 0.1  # Learning rate\n",
        "n_iterations = 1000\n",
        "m = 100 # Jumlah instance\n",
        "theta_bgd = np.random.randn(2, 1)  # Inisialisasi acak\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    # Hitung gradien (turunan parsial dari MSE)\n",
        "    gradients = 2/m * X_b.T.dot(X_b.dot(theta_bgd) - y)\n",
        "    # Update theta (melangkah \"menuruni\" bukit)\n",
        "    theta_bgd = theta_bgd - eta * gradients\n",
        "\n",
        "print(f\"  Theta terbaik (dari Batch GD): \\n{theta_bgd}\")\n",
        "\n",
        "# Varian 2: Stochastic Gradient Descent (SGD)\n",
        "# SGD mengambil *satu* instance acak pada setiap langkah.\n",
        "# Jauh lebih cepat, tapi kurang stabil (stochastic/acak).\n",
        "# Perlu \"learning schedule\" untuk mengurangi 'eta' seiring waktu.\n",
        "print(\"  Melatih dengan Stochastic GD (SGD)...\")\n",
        "n_epochs = 50\n",
        "t0, t1 = 5, 50  # Hyperparameter untuk learning schedule\n",
        "\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "theta_sgd = np.random.randn(2, 1)  # Inisialisasi acak\n",
        "\n",
        "for epoch in range(n_epochs): # Epoch = satu putaran penuh data\n",
        "    for i in range(m):\n",
        "        random_index = np.random.randint(m)\n",
        "        xi = X_b[random_index:random_index+1]\n",
        "        yi = y[random_index:random_index+1]\n",
        "\n",
        "        gradients = 2 * xi.T.dot(xi.dot(theta_sgd) - yi)\n",
        "        eta = learning_schedule(epoch * m + i)\n",
        "        theta_sgd = theta_sgd - eta * gradients\n",
        "\n",
        "print(f\"  Theta terbaik (dari SGD): \\n{theta_sgd}\")\n",
        "\n",
        "# Varian 3: Mini-batch Gradient Descent (MGD)\n",
        "# Kompromi: menghitung gradien pada 'mini-batch' (sekelompok kecil data).\n",
        "# Ini adalah yang paling umum digunakan dalam Deep Learning.\n",
        "\n",
        "# Scikit-Learn SGDRegressor mengimplementasikan SGD/MGD\n",
        "# (Secara default MGD karena batch_size default tidak 1)\n",
        "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)\n",
        "# y.ravel() mengubah y dari [100, 1] menjadi [100,]\n",
        "sgd_reg.fit(X, y.ravel())\n",
        "print(f\"  Theta terbaik (dari SGDRegressor SK-Learn): \\n  Intercept: {sgd_reg.intercept_}, Coef: {sgd_reg.coef_}\")\n",
        "\n",
        "\n",
        "# 2. REGRESI POLINOMIAL\n",
        "print(\"\\n[2. Regresi Polinomial]\")\n",
        "# Teori: Kita bisa menggunakan model linear untuk data non-linear\n",
        "# dengan cara menambahkan fitur baru yang merupakan pangkat dari fitur asli.\n",
        "\n",
        "# Membuat data non-linear (kuadratik)\n",
        "m = 100\n",
        "X_poly_data = 6 * np.random.rand(m, 1) - 3\n",
        "y_poly_data = 0.5 * X_poly_data**2 + X_poly_data + 2 + np.random.randn(m, 1)\n",
        "\n",
        "# Plot data non-linear (di-comment)\n",
        "# plt.plot(X_poly_data, y_poly_data, \"b.\")\n",
        "# plt.xlabel(\"$x_1$\")\n",
        "# plt.ylabel(\"$y$\")\n",
        "# plt.title(\"Data Non-Linear\")\n",
        "# plt.axis([-3, 3, 0, 10])\n",
        "# plt.show()\n",
        "\n",
        "# Gunakan Scikit-Learn PolynomialFeatures\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly_transformed = poly_features.fit_transform(X_poly_data)\n",
        "\n",
        "# X_poly_transformed sekarang punya 2 fitur: x1 dan x1^2\n",
        "# print(f\"  Fitur asli: {X_poly_data[0]}\")\n",
        "# print(f\"  Fitur baru (polinomial): {X_poly_transformed[0]}\")\n",
        "\n",
        "# Latih LinearRegression pada data yang sudah ditransformasi\n",
        "lin_reg_poly = LinearRegression()\n",
        "lin_reg_poly.fit(X_poly_transformed, y_poly_data)\n",
        "print(f\"  Theta (polinomial): Intercept: {lin_reg_poly.intercept_}, Coef: {lin_reg_poly.coef_}\")\n",
        "# Hasilnya harusnya dekat dengan [2] (intercept), [1] (coef x1), [0.5] (coef x1^2)\n",
        "\n",
        "\n",
        "# 3. KURVA BELAJAR (LEARNING CURVES)\n",
        "print(\"\\n[3. Kurva Belajar (Learning Curves)]\")\n",
        "# Teori: Kurva Belajar memplot error latih & error validasi\n",
        "# sebagai fungsi dari ukuran data latih.\n",
        "# Ini adalah cara utama untuk mendiagnosis Overfitting atau Underfitting.\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    # Membagi data untuk validasi\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
        "    train_errors, val_errors = [], []\n",
        "\n",
        "    for m in range(1, len(X_train)): # Iterasi dari 1 instance s/d semua\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_val_predict = model.predict(X_val)\n",
        "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
        "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"Train\")\n",
        "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Validation\")\n",
        "    plt.legend(loc=\"upper right\", fontsize=14)\n",
        "    plt.xlabel(\"Training set size\", fontsize=14)\n",
        "    plt.ylabel(\"RMSE\", fontsize=14)\n",
        "    plt.axis([0, 80, 0, 3])\n",
        "\n",
        "# Contoh 1: Underfitting (Model terlalu sederhana)\n",
        "# lin_reg_curve = LinearRegression()\n",
        "# plot_learning_curves(lin_reg_curve, X_poly_data, y_poly_data)\n",
        "# plt.title(\"Kurva Belajar (Underfitting)\")\n",
        "# plt.show()\n",
        "# Hasil: Kedua error bertemu di plato pada level error yang TINGGI.\n",
        "\n",
        "# Contoh 2: Overfitting (Model terlalu kompleks)\n",
        "# polynomial_regression = Pipeline([\n",
        "#     (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
        "#     (\"lin_reg\", LinearRegression()),\n",
        "# ])\n",
        "# plot_learning_curves(polynomial_regression, X_poly_data, y_poly_data)\n",
        "# plt.title(\"Kurva Belajar (Overfitting)\")\n",
        "# plt.show()\n",
        "# Hasil: Ada JARAK (gap) besar antara error latih (rendah) dan error validasi (tinggi).\n",
        "\n",
        "# 4. MODEL LINEAR YANG DIREGULARISASI\n",
        "print(\"\\n[4. Model Linear yang Diregularisasi]\")\n",
        "# Teori: Regularisasi adalah cara untuk membatasi (constrain)\n",
        "# model untuk mencegah overfitting.\n",
        "\n",
        "# 4a. Ridge Regression (Penalti L2)\n",
        "# Menambahkan penalty L2 (setengah kuadrat dari norma L2 bobot) ke cost function.\n",
        "# Ini memaksa bobot (weights) tetap kecil.\n",
        "from sklearn.linear_model import Ridge\n",
        "# alpha = kekuatan regularisasi. alpha=0 sama dengan Linear Regression.\n",
        "ridge_reg = Ridge(alpha=1, solver=\"cholesky\", random_state=42)\n",
        "ridge_reg.fit(X, y)\n",
        "# print(ridge_reg.predict([[1.5]]))\n",
        "\n",
        "# 4b. Lasso Regression (Penalti L1)\n",
        "# Menambahkan penalty L1 (norma L1 dari bobot) ke cost function.\n",
        "# Efek utamanya: cenderung menghilangkan bobot fitur yang tidak penting (menjadi 0).\n",
        "# Bagus untuk feature selection otomatis.\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso_reg = Lasso(alpha=0.1)\n",
        "lasso_reg.fit(X, y)\n",
        "# print(lasso_reg.predict([[1.5]]))\n",
        "\n",
        "# 4c. Elastic Net\n",
        "# Gabungan dari Ridge dan Lasso.\n",
        "# Dikontrol oleh 'l1_ratio' (r). Jika r=1, ini Lasso. Jika r=0, ini Ridge.\n",
        "from sklearn.linear_model import ElasticNet\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
        "elastic_net.fit(X, y)\n",
        "# print(elastic_net.predict([[1.5]]))\n",
        "\n",
        "# 4d. Early Stopping (Penghentian Dini)\n",
        "# Teori: Bentuk regularisasi yang sangat berbeda.\n",
        "# Hentikan pelatihan segera setelah performa di validation set\n",
        "# berhenti membaik (mulai naik).\n",
        "print(\"  Implementasi Early Stopping (konseptual)...\")\n",
        "# (Kode implementasi manual Early Stopping akan panjang,\n",
        "#  jadi kita hanya tunjukkan konsep dasarnya)\n",
        "# (Kita tiru data dari buku)\n",
        "np.random.seed(42)\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)\n",
        "\n",
        "# Siapkan data\n",
        "poly_scaler = Pipeline([\n",
        "    (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
        "    (\"std_scaler\", StandardScaler())\n",
        "])\n",
        "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
        "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
        "\n",
        "sgd_reg = SGDRegressor(max_iter=1, tol=-np.inf, warm_start=True,\n",
        "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "minimum_val_error = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_model = None\n",
        "\n",
        "# for epoch in range(1000):\n",
        "#     sgd_reg.fit(X_train_poly_scaled, y_train)  # warm_start=True melanjutkan latihan\n",
        "#     y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "#     val_error = mean_squared_error(y_val, y_val_predict)\n",
        "#     if val_error < minimum_val_error:\n",
        "#         minimum_val_error = val_error\n",
        "#         best_epoch = epoch\n",
        "#         best_model = clone(sgd_reg) # Simpan model terbaik\n",
        "\n",
        "# print(f\"  Epoch terbaik: {best_epoch}\")\n",
        "# print(f\"  Error validasi minimum: {minimum_val_error}\")\n",
        "\n",
        "\n",
        "# 5. LOGISTIC & SOFTMAX REGRESSION\n",
        "print(\"\\n[5. Regresi Logistik dan Softmax]\")\n",
        "# Teori: Logistic Regression adalah untuk klasifikasi biner.\n",
        "# Ia menghitung probabilitas menggunakan fungsi sigmoid (logistik).\n",
        "\n",
        "# Menggunakan dataset Iris\n",
        "iris = datasets.load_iris()\n",
        "X_iris = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y_iris = iris[\"target\"]\n",
        "\n",
        "# 5a. Logistic Regression (Binary)\n",
        "# Kita buat target biner: Apakah Iris virginica (kelas 2)?\n",
        "y_iris_virginica = (y_iris == 2).astype(np.int64)\n",
        "\n",
        "log_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "log_reg.fit(X_iris, y_iris_virginica)\n",
        "\n",
        "# X_new_iris = np.array([[1.7, 0.5], [5, 2]])\n",
        "# print(f\"  Prediksi (biner): {log_reg.predict(X_new_iris)}\")\n",
        "# print(f\"  Probabilitas (biner): \\n{log_reg.predict_proba(X_new_iris)}\")\n",
        "\n",
        "\n",
        "# 5b. Softmax Regression (Multiclass)\n",
        "# Teori: Generalisasi Logistic Regression untuk banyak kelas.\n",
        "# Menggunakan fungsi softmax untuk memastikan probabilitas semua kelas berjumlah 1.\n",
        "# Cukup atur multi_class=\"multinomial\"\n",
        "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, random_state=42)\n",
        "softmax_reg.fit(X_iris, y_iris) # Latih pada semua 3 kelas\n",
        "\n",
        "print(f\"  Prediksi Softmax (multikelas) untuk [5, 2]: {softmax_reg.predict([[5, 2]])}\")\n",
        "print(f\"  Probabilitas Softmax (multikelas) untuk [5, 2]: \\n{softmax_reg.predict_proba([[5, 2]])}\")\n",
        "\n",
        "print(\"\\n--- Selesai Bab 4 ---\")"
      ]
    }
  ]
}