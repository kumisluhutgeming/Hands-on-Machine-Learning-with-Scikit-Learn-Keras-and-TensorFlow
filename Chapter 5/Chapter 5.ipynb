{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a778cVvmAk-r",
        "outputId": "050fc834-d8ec-4eb2-83c7-a2fa88f923de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- BAB 5: SUPPORT VECTOR MACHINES (SVM) ---\n",
            "Semua library telah diimpor.\n",
            "\n",
            "[1. Klasifikasi SVM Linear]\n",
            "Model LinearSVC telah dilatih.\n",
            "Prediksi untuk [5.5, 1.7]: [1.]\n",
            "\n",
            "[2. Klasifikasi SVM Non-Linear]\n",
            "  Melatih SVM Non-Linear dengan Fitur Polinomial...\n",
            "  Model SVM dengan Fitur Polinomial (degree 3) telah dilatih.\n",
            "  Melatih SVM Non-Linear dengan Kernel Trick...\n",
            "  Model SVM dengan Kernel 'poly' telah dilatih.\n",
            "  Model SVM dengan Kernel 'rbf' telah dilatih.\n",
            "\n",
            "[3. Regresi SVM (SVR)]\n",
            "  Melatih LinearSVR...\n",
            "  Model LinearSVR (epsilon=1.5) telah dilatih.\n",
            "  Melatih SVR Non-Linear (kernel poly)...\n",
            "  Model SVR Non-Linear (degree=2) telah dilatih.\n",
            "\n",
            "--- Selesai Bab 5 ---\n"
          ]
        }
      ],
      "source": [
        "# MENGIMPOR SEMUA LIBRARY YANG DIPERLUKAN UNTUK BAB 5\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.svm import LinearSVC, SVC, LinearSVR, SVR\n",
        "\n",
        "# Mengatur default plotting\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Untuk plot di dalam notebook (jika Anda menggunakan Jupyter)\n",
        "# %matplotlib inline\n",
        "\n",
        "print(\"--- BAB 5: SUPPORT VECTOR MACHINES (SVM) ---\")\n",
        "print(\"Semua library telah diimpor.\")\n",
        "\n",
        "# 1. KLASIFIKASI SVM LINEAR\n",
        "print(\"\\n[1. Klasifikasi SVM Linear]\")\n",
        "# Teori: Ide utama SVM adalah 'Large Margin Classification'.\n",
        "# SVM tidak hanya mencari garis untuk memisahkan kelas, tetapi mencari\n",
        "# \"jalan\" (margin) terluas di antara kelas.\n",
        "#\n",
        "# - Hard Margin: Semua data HARUS di luar jalan. Sensitif outlier.\n",
        "# - Soft Margin: Keseimbangan antara jalan lebar & 'pelanggaran margin'\n",
        "#   (data yang masuk ke jalan). Ini dikontrol oleh hyperparameter 'C'.\n",
        "#   - C rendah: Jalan lebar, lebih banyak pelanggaran (regularisasi tinggi).\n",
        "#   - C tinggi: Jalan sempit, lebih sedikit pelanggaran (regularisasi rendah).\n",
        "\n",
        "# Menggunakan dataset Iris\n",
        "iris = datasets.load_iris()\n",
        "X = iris[\"data\"][:, (2, 3)]  # fitur: petal length, petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.float64)  # target: 1 jika Iris virginica, 0 jika bukan\n",
        "\n",
        "# PENTING: SVM SANGAT SENSITIF TERHADAP SKALA FITUR\n",
        "# Kita harus menggunakan StandardScaler!\n",
        "# Kita gunakan Pipeline untuk menggabungkan scaling dan model SVM.\n",
        "\n",
        "# LinearSVC adalah implementasi SVM linear yang cepat.\n",
        "# 'hinge' adalah loss function standar untuk SVM.\n",
        "# max_iter disetel tinggi untuk memastikan konvergensi\n",
        "svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42, dual=True, max_iter=10000))\n",
        "    ])\n",
        "\n",
        "svm_clf.fit(X, y)\n",
        "\n",
        "print(\"Model LinearSVC telah dilatih.\")\n",
        "print(f\"Prediksi untuk [5.5, 1.7]: {svm_clf.predict([[5.5, 1.7]])}\") # Akan memprediksi [1.]\n",
        "\n",
        "# 2. KLASIFIKASI SVM NON-LINEAR\n",
        "print(\"\\n[2. Klasifikasi SVM Non-Linear]\")\n",
        "# Banyak dataset tidak dapat dipisahkan secara linear.\n",
        "\n",
        "# Menggunakan dataset Moons\n",
        "from sklearn.datasets import make_moons\n",
        "X_moons, y_moons = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
        "\n",
        "# Fungsi helper untuk plot\n",
        "def plot_dataset(X, y, axes):\n",
        "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
        "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
        "    plt.axis(axes)\n",
        "    plt.grid(True, which='both')\n",
        "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
        "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n",
        "\n",
        "# plot_dataset(X_moons, y_moons, [-1.5, 2.5, -1, 1.5])\n",
        "# plt.title(\"Dataset Moons (Non-Linear)\")\n",
        "# plt.show() # Data ini tidak bisa dipisah garis lurus\n",
        "\n",
        "# 2a. Metode 1: Fitur Polinomial\n",
        "# Teori: Kita bisa menambahkan fitur baru (misal, x1^2, x2^2, x1*x2)\n",
        "# untuk membuat data dapat dipisahkan secara linear di dimensi yang lebih tinggi.\n",
        "print(\"  Melatih SVM Non-Linear dengan Fitur Polinomial...\")\n",
        "\n",
        "polynomial_svm_clf = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42, dual=True, max_iter=10000))\n",
        "    ])\n",
        "\n",
        "polynomial_svm_clf.fit(X_moons, y_moons)\n",
        "print(\"  Model SVM dengan Fitur Polinomial (degree 3) telah dilatih.\")\n",
        "\n",
        "# 2b. Metode 2: Kernel Trick (Cara yang Jauh Lebih Efisien)\n",
        "print(\"  Melatih SVM Non-Linear dengan Kernel Trick...\")\n",
        "# Teori: 'Kernel Trick' adalah trik matematika yang memungkinkan SVM\n",
        "# mendapatkan hasil yang SAMA SEPERTI menggunakan fitur polinomial\n",
        "# berderajat tinggi, tanpa benar-benar HARUS menambahkannya.\n",
        "# Ini menghemat biaya komputasi secara drastis.\n",
        "\n",
        "# Kernel Polinomial\n",
        "poly_kernel_svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
        "    ])\n",
        "poly_kernel_svm_clf.fit(X_moons, y_moons)\n",
        "print(\"  Model SVM dengan Kernel 'poly' telah dilatih.\")\n",
        "\n",
        "# Kernel Gaussian RBF (Radial Basis Function)\n",
        "# Teori: Kernel RBF adalah kernel yang paling populer dan kuat.\n",
        "# Ia bekerja dengan menambahkan fitur berdasarkan 'kedekatan'\n",
        "# (similarity) setiap instance ke 'landmarks'.\n",
        "#\n",
        "# Hyperparameter utama:\n",
        "# - 'C': Mengontrol soft margin (sama seperti LinearSVC).\n",
        "# - 'gamma': Mengontrol 'lebar' kurva Gaussian.\n",
        "#   - gamma kecil: Kurva lebar, decision boundary halus (regularisasi tinggi).\n",
        "#   - gamma besar: Kurva sempit, boundary berliku (regularisasi rendah, bisa overfit).\n",
        "rbf_kernel_svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001)) # Contoh gamma besar, C kecil\n",
        "    ])\n",
        "rbf_kernel_svm_clf.fit(X_moons, y_moons)\n",
        "print(\"  Model SVM dengan Kernel 'rbf' telah dilatih.\")\n",
        "\n",
        "\n",
        "# 3. REGRESI SVM (SVR)\n",
        "print(\"\\n[3. Regresi SVM (SVR)]\")\n",
        "# Teori: SVM juga bisa untuk regresi. Idenya dibalik:\n",
        "# Alih-alih mencari \"jalan\" terluas DI ANTARA kelas,\n",
        "# SVR mencari \"jalan\" (margin) yang bisa memuat SEBANYAK MUNGKIN\n",
        "# instance DI DALAM jalan.\n",
        "# Lebar jalan dikontrol oleh hyperparameter 'epsilon' (e).\n",
        "\n",
        "# 3a. Linear SVR\n",
        "print(\"  Melatih LinearSVR...\")\n",
        "# Membuat data linear palsu\n",
        "np.random.seed(42)\n",
        "m_reg = 50\n",
        "X_reg = 2 * np.random.rand(m_reg, 1)\n",
        "y_reg = (4 + 3 * X_reg + np.random.randn(m_reg, 1)).ravel() # .ravel() mengubah ke 1D\n",
        "\n",
        "# 'epsilon' mengontrol lebar jalan (margin)\n",
        "linear_svr = LinearSVR(epsilon=1.5, random_state=42, max_iter=10000)\n",
        "linear_svr.fit(X_reg, y_reg)\n",
        "print(\"  Model LinearSVR (epsilon=1.5) telah dilatih.\")\n",
        "\n",
        "# 3b. Nonlinear SVR (menggunakan Kernel Trick)\n",
        "print(\"  Melatih SVR Non-Linear (kernel poly)...\")\n",
        "# Membuat data kuadratik palsu\n",
        "m_reg_nl = 100\n",
        "X_reg_nl = 2 * np.random.rand(m_reg_nl, 1) - 1\n",
        "y_reg_nl = (0.2 + 0.1 * X_reg_nl + 0.5 * X_reg_nl**2 + np.random.randn(m_reg_nl, 1) / 10).ravel()\n",
        "\n",
        "# Menggunakan SVR dengan kernel polinomial\n",
        "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
        "svm_poly_reg.fit(X_reg_nl, y_reg_nl)\n",
        "print(\"  Model SVR Non-Linear (degree=2) telah dilatih.\")\n",
        "\n",
        "# Plot hasil SVR Non-Linear (di-comment)\n",
        "# X_new_nl = np.linspace(-1, 1, 100).reshape(100, 1)\n",
        "# y_pred_nl = svm_poly_reg.predict(X_new_nl)\n",
        "# plt.plot(X_reg_nl, y_reg_nl, \"b.\")\n",
        "# plt.plot(X_new_nl, y_pred_nl, \"r-\", linewidth=2, label=\"Prediksi SVR\")\n",
        "# plt.xlabel(\"$x_1$\")\n",
        "# plt.ylabel(\"$y$\")\n",
        "# plt.legend()\n",
        "# plt.title(\"SVR dengan Kernel Polinomial\")\n",
        "# plt.show()\n",
        "\n",
        "print(\"\\n--- Selesai Bab 5 ---\")"
      ]
    }
  ]
}