{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4-Uq-DgFP4M",
        "outputId": "9f8b8760-1eab-46ca-a983-f2511a7fc835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- BAB 8: PENGURANGAN DIMENSI (DIMENSIONALITY REDUCTION) ---\n",
            "Semua library telah diimpor.\n",
            "\n",
            "[1. Principal Component Analysis (PCA)]\n",
            "  Menghitung PCA secara manual dengan SVD...\n",
            "  PC 1: [0.93636116 0.29854881 0.18465208]\n",
            "  PC 2: [-0.34027485  0.90119108  0.2684542 ]\n",
            "  Melakukan PCA dengan Scikit-Learn...\n",
            "  Komponen (PC) dari SK-Learn:\n",
            "[[ 0.93636116 -0.34027485]\n",
            " [ 0.29854881  0.90119108]\n",
            " [ 0.18465208  0.2684542 ]]\n",
            "  Menghitung Explained Variance Ratio...\n",
            "  Rasio Varians: [0.84248607 0.14631839]\n",
            "  Memilih jumlah dimensi untuk MNIST...\n",
            "  Dimensi yang dibutuhkan untuk 95% varians: 154 (dari 784)\n",
            "  Bentuk data setelah reduksi 95%: (60000, 154)\n",
            "  Menggunakan PCA untuk kompresi MNIST...\n",
            "  Melatih Randomized PCA...\n",
            "  Melatih Incremental PCA (IPCA)...\n",
            "\n",
            "[2. Kernel PCA (kPCA)]\n",
            "  Tuning hyperparameter kPCA menggunakan GridSearchCV...\n",
            "  Hyperparameter kPCA terbaik (hasil GridSearchCV): {'kpca__gamma': np.float64(0.05), 'kpca__kernel': 'rbf'}\n",
            "  Melatih kPCA dengan dukungan inversi...\n",
            "  Reconstruction Pre-image Error: 32.7863\n",
            "\n",
            "[3. LLE (Locally Linear Embedding)]\n",
            "\n",
            "[4. Teknik Lainnya (MDS, Isomap, t-SNE, LDA)]\n",
            "\n",
            "--- Selesai Bab 8 ---\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Impor untuk PCA dan variannya\n",
        "from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA\n",
        "\n",
        "# Impor untuk Manifold Learning\n",
        "from sklearn.manifold import LocallyLinearEmbedding, TSNE\n",
        "\n",
        "# Impor dataset\n",
        "from sklearn.datasets import fetch_openml, make_swiss_roll\n",
        "\n",
        "# Impor untuk pipeline dan evaluasi\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Mengatur default plotting\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "print(\"--- BAB 8: PENGURANGAN DIMENSI (DIMENSIONALITY REDUCTION) ---\")\n",
        "print(\"Semua library telah diimpor.\")\n",
        "\n",
        "# 1. PRINCIPAL COMPONENT ANALYSIS (PCA)\n",
        "print(\"\\n[1. Principal Component Analysis (PCA)]\")\n",
        "# Teori: PCA adalah algoritma Proyeksi (Projection).\n",
        "# Ia mengidentifikasi hyperplane yang paling dekat dengan data,\n",
        "# lalu memproyeksikan data ke hyperplane tersebut[cite: 1124].\n",
        "# Tujuannya adalah untuk mempertahankan 'variance' (keragaman) data sebanyak mungkin[cite: 1125].\n",
        "\n",
        "# 1a. Konsep: PCA dengan SVD (Singular Value Decomposition)\n",
        "print(\"  Menghitung PCA secara manual dengan SVD...\")\n",
        "# Membuat data 3D sintetis yang hampir datar (terletak di bidang 2D)\n",
        "np.random.seed(4)\n",
        "m = 60\n",
        "w1, w2 = 0.1, 0.3\n",
        "noise = 0.1\n",
        "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
        "X_3d = np.empty((m, 3))\n",
        "X_3d[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
        "X_3d[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
        "X_3d[:, 2] = X_3d[:, 0] * w1 + X_3d[:, 1] * w2 + noise * np.random.randn(m)\n",
        "\n",
        "# Langkah 1: Pusatkan data (mean = 0)\n",
        "X_3d_centered = X_3d - X_3d.mean(axis=0)\n",
        "\n",
        "# Langkah 2: Gunakan SVD untuk mendapatkan semua Principal Components\n",
        "U, s, Vt = np.linalg.svd(X_3d_centered)\n",
        "\n",
        "# Principal Components (PC) adalah vektor W\n",
        "# Vt adalah V transpose\n",
        "c1 = Vt.T[:, 0]\n",
        "c2 = Vt.T[:, 1]\n",
        "print(f\"  PC 1: {c1}\\n  PC 2: {c2}\")\n",
        "\n",
        "# Langkah 3: Proyeksikan data ke 2D (ke bidang yang dibentuk oleh 2 PC pertama)\n",
        "W2 = Vt.T[:, :2]\n",
        "X_2d_svd = X_3d_centered.dot(W2)\n",
        "\n",
        "# 1b. PCA menggunakan Scikit-Learn\n",
        "print(\"  Melakukan PCA dengan Scikit-Learn...\")\n",
        "# Scikit-Learn otomatis menangani pemusatan data (centering)\n",
        "pca = PCA(n_components=2)\n",
        "X_2d_sklearn = pca.fit_transform(X_3d)\n",
        "\n",
        "# Membandingkan hasil SVD manual dan Scikit-Learn\n",
        "# (Bisa terbalik, misal c1 = -c1_sklearn, tapi bidangnya sama)\n",
        "print(f\"  Komponen (PC) dari SK-Learn:\\n{pca.components_.T}\")\n",
        "\n",
        "# 1c. Explained Variance Ratio (Rasio Varians yang Dijelaskan)\n",
        "print(\"  Menghitung Explained Variance Ratio...\")\n",
        "# Teori: Ini memberi tahu kita berapa persen varians data\n",
        "# yang terletak di sepanjang setiap Principal Component.\n",
        "print(f\"  Rasio Varians: {pca.explained_variance_ratio_}\")\n",
        "# Hasilnya menunjukkan bahwa PC1 menampung sebagian besar varians,\n",
        "# PC2 menampung sisa varians (total ~100%), dan PC3 (yang tidak kita pilih)\n",
        "# menampung hampir 0% varians.\n",
        "\n",
        "# 1d. Memilih Jumlah Dimensi yang Tepat\n",
        "print(\"  Memilih jumlah dimensi untuk MNIST...\")\n",
        "# Mari kita gunakan dataset MNIST\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
        "X_mnist = mnist[\"data\"]\n",
        "y_mnist = mnist[\"target\"].astype(np.uint8)\n",
        "\n",
        "X_train_mnist = X_mnist[:60000]\n",
        "y_train_mnist = y_mnist[:60000]\n",
        "\n",
        "# Latih PCA tanpa mengurangi dimensi untuk melihat semua varians\n",
        "pca_mnist = PCA()\n",
        "pca_mnist.fit(X_train_mnist)\n",
        "\n",
        "# Hitung jumlah kumulatif dari varians\n",
        "cumsum = np.cumsum(pca_mnist.explained_variance_ratio_)\n",
        "\n",
        "# Cari jumlah dimensi yang diperlukan untuk 95% varians\n",
        "d = np.argmax(cumsum >= 0.95) + 1\n",
        "print(f\"  Dimensi yang dibutuhkan untuk 95% varians: {d} (dari 784)\")\n",
        "\n",
        "# Cara lebih mudah: jalankan PCA dengan n_components=float (0.0 - 1.0)\n",
        "pca_95 = PCA(n_components=0.95)\n",
        "X_mnist_reduced = pca_95.fit_transform(X_train_mnist)\n",
        "print(f\"  Bentuk data setelah reduksi 95%: {X_mnist_reduced.shape}\")\n",
        "\n",
        "# 1e. PCA untuk Kompresi (dan Dekompresi)\n",
        "print(\"  Menggunakan PCA untuk kompresi MNIST...\")\n",
        "# Setelah reduksi, kita bisa dekompresi kembali ke 784D\n",
        "# Ini akan kehilangan sedikit informasi (5% varians),\n",
        "# tapi harusnya masih mirip. Ini disebut 'reconstruction error'.\n",
        "X_mnist_recovered = pca_95.inverse_transform(X_mnist_reduced)\n",
        "\n",
        "# Fungsi helper untuk plot\n",
        "def plot_digits(instances, images_per_row=5, **options):\n",
        "    size = 28\n",
        "    images_per_row = min(len(instances), images_per_row)\n",
        "    images = [instance.reshape(size,size) for instance in instances]\n",
        "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
        "    row_images = []\n",
        "    n_empty = n_rows * images_per_row - len(instances)\n",
        "    images.append(np.zeros((size, size * n_empty)))\n",
        "    for row in range(n_rows):\n",
        "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
        "        row_images.append(np.concatenate(rimages, axis=1))\n",
        "    image = np.concatenate(row_images, axis=0)\n",
        "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "# Plot perbandingan (di-comment)\n",
        "# plt.figure(figsize=(7, 4))\n",
        "# plt.subplot(121)\n",
        "# plot_digits(X_train_mnist[::2100])\n",
        "# plt.title(\"Asli\", fontsize=16)\n",
        "# plt.subplot(122)\n",
        "# plot_digits(X_mnist_recovered[::2100])\n",
        "# plt.title(\"Telah Dikompresi (95%)\", fontsize=16)\n",
        "# plt.show()\n",
        "\n",
        "# 1f. Randomized PCA\n",
        "# Scikit-Learn memiliki 'Randomized PCA' yang jauh lebih cepat\n",
        "# Bekerja baik jika d (target dimensi) jauh lebih kecil dari n (fitur asli)\n",
        "print(\"  Melatih Randomized PCA...\")\n",
        "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\", random_state=42)\n",
        "X_mnist_reduced_rnd = rnd_pca.fit_transform(X_train_mnist)\n",
        "# Hasilnya sama, tapi jauh lebih cepat.\n",
        "\n",
        "# 1g. Incremental PCA (IPCA)\n",
        "# Teori: Berguna untuk dataset besar (out-of-core) atau online learning.\n",
        "# IPCA memecah data latih menjadi mini-batches dan melatihnya\n",
        "# satu per satu menggunakan 'partial_fit()'.\n",
        "print(\"  Melatih Incremental PCA (IPCA)...\")\n",
        "n_batches = 100\n",
        "inc_pca = IncrementalPCA(n_components=154)\n",
        "for X_batch in np.array_split(X_train_mnist, n_batches):\n",
        "    inc_pca.partial_fit(X_batch)\n",
        "\n",
        "X_mnist_reduced_inc = inc_pca.transform(X_train_mnist)\n",
        "# Hasilnya akan sangat mirip dengan PCA biasa.\n",
        "\n",
        "# 2. KERNEL PCA (kPCA)\n",
        "print(\"\\n[2. Kernel PCA (kPCA)]\")\n",
        "# Teori: kPCA menggunakan 'Kernel Trick' (dari Bab 5)\n",
        "# untuk melakukan reduksi dimensi non-linear yang kompleks.\n",
        "# Berguna untuk 'membuka' manifold yang terpilin.\n",
        "\n",
        "X_swiss, y_swiss_continuous = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
        "# Convert continuous y_swiss to discrete labels for classification\n",
        "y_swiss = np.array(y_swiss_continuous > np.median(y_swiss_continuous), dtype=int)\n",
        "\n",
        "\n",
        "# Melatih kPCA dengan kernel RBF\n",
        "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04, random_state=42)\n",
        "X_reduced_kpca = rbf_pca.fit_transform(X_swiss)\n",
        "\n",
        "# Plot hasil kPCA (di-comment)\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.scatter(X_reduced_kpca[:, 0], X_reduced_kpca[:, 1], c=y_swiss, cmap=plt.cm.hot)\n",
        "# plt.title(\"kPCA (RBF) membuka Swiss Roll\")\n",
        "# plt.xlabel(\"$z_1$\")\n",
        "# plt.ylabel(\"$z_2$\")\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "# 2a. Memilih Kernel dan Tuning Hyperparameter\n",
        "print(\"  Tuning hyperparameter kPCA menggunakan GridSearchCV...\")\n",
        "# Teori: Karena kPCA adalah unsupervised, kita tidak bisa\n",
        "# men-tuningnya berdasarkan 'akurasi'.\n",
        "# TAPI, kita bisa membuat pipeline: 1. kPCA, 2. Klasifikasi.\n",
        "# Lalu kita cari hyperparameter kPCA yang memberikan akurasi klasifikasi terbaik.\n",
        "\n",
        "clf_kpca = Pipeline([\n",
        "    (\"kpca\", KernelPCA(n_components=2)),\n",
        "    (\"log_reg\", LogisticRegression(solver=\"lbfgs\", max_iter=1000)) # Added max_iter for convergence\n",
        "])\n",
        "\n",
        "param_grid = [{\n",
        "    \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
        "    \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
        "}]\n",
        "\n",
        "grid_search = GridSearchCV(clf_kpca, param_grid, cv=3)\n",
        "grid_search.fit(X_swiss, y_swiss)\n",
        "\n",
        "print(f\"  Hyperparameter kPCA terbaik (hasil GridSearchCV): {grid_search.best_params_}\")\n",
        "\n",
        "# 2b. kPCA: Rekonstruksi (Pre-image)\n",
        "# Teori: Merekonstruksi data dari kPCA itu sulit.\n",
        "# Kita bisa melatih 'inversi' aproksimatif.\n",
        "print(\"  Melatih kPCA dengan dukungan inversi...\")\n",
        "rbf_pca_recon = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.0433,\n",
        "                          fit_inverse_transform=True, random_state=42)\n",
        "X_reduced_recon = rbf_pca_recon.fit_transform(X_swiss)\n",
        "X_preimage = rbf_pca_recon.inverse_transform(X_reduced_recon)\n",
        "\n",
        "# Hitung reconstruction error\n",
        "recon_error = mean_squared_error(X_swiss, X_preimage)\n",
        "print(f\"  Reconstruction Pre-image Error: {recon_error:.4f}\")\n",
        "\n",
        "\n",
        "# 3. LLE (LOCALLY LINEAR EMBEDDING)\n",
        "print(\"\\n[3. LLE (Locally Linear Embedding)]\")\n",
        "# Teori: LLE adalah algoritma Manifold Learning.\n",
        "# LLE tidak memproyeksikan data, tetapi 'membuka' manifold.\n",
        "# Ia bekerja dengan melihat hubungan linear lokal setiap instance\n",
        "# dengan tetangga terdekatnya, lalu mencoba mempertahankan\n",
        "# hubungan lokal tersebut di ruang berdimensi rendah.\n",
        "\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
        "X_reduced_lle = lle.fit_transform(X_swiss)\n",
        "\n",
        "# Plot hasil LLE (di-comment)\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.scatter(X_reduced_lle[:, 0], X_reduced_lle[:, 1], c=y_swiss, cmap=plt.cm.hot)\n",
        "# plt.title(\"LLE membuka Swiss Roll\")\n",
        "# plt.xlabel(\"$z_1$\")\n",
        "# plt.ylabel(\"$z_2$\")\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "# Hasilnya: Swiss roll terbuka, tapi skalanya sedikit terdistorsi.\n",
        "\n",
        "# 4. TEKNIK LAINNYA (Disebutkan di buku)\n",
        "print(\"\\n[4. Teknik Lainnya (MDS, Isomap, t-SNE, LDA)]\")\n",
        "# - Multidimensional Scaling (MDS): Mencoba mempertahankan jarak antar instance.\n",
        "# - Isomap: Menjaga 'geodesic distance' (jarak berjalan di atas manifold).\n",
        "# - t-SNE: Sangat baik untuk visualisasi (terutama clustering),\n",
        "#   ia menjaga instance serupa tetap dekat dan instance tidak serupa tetap jauh.\n",
        "# - LDA (Linear Discriminant Analysis): Sebenarnya algoritma klasifikasi,\n",
        "#   tapi ia belajar sumbu (axes) yang paling baik 'memisahkan' kelas,\n",
        "#   yang bisa digunakan untuk reduksi dimensi.\n",
        "\n",
        "print(\"\\n--- Selesai Bab 8 ---\")"
      ]
    }
  ]
}