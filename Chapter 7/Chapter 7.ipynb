{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a778cVvmAk-r",
        "outputId": "ca942ebd-9a9e-41db-af86-8dcb7d05ae34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- BAB 7: ENSEMBLE LEARNING DAN RANDOM FORESTS ---\n",
            "Semua library telah diimpor.\n",
            "\n",
            "[1. Voting Classifiers]\n",
            "  Akurasi masing-masing model (Hard Voting):\n",
            "  - LogisticRegression: 0.8640\n",
            "  - RandomForestClassifier: 0.8960\n",
            "  - SVC: 0.8960\n",
            "  - VotingClassifier: 0.9120\n",
            "\n",
            "  Akurasi (Soft Voting):\n",
            "  - VotingClassifier (Soft): 0.9200\n",
            "\n",
            "[2. Bagging dan Pasting]\n",
            "  Akurasi Decision Tree tunggal: 0.8560\n",
            "  Akurasi Bagging (500 pohon): 0.9040\n",
            "\n",
            "[3. Out-of-Bag (OOB) Evaluation]\n",
            "  Skor OOB: 0.8987\n",
            "  Akurasi Test Set: 0.9120\n",
            "\n",
            "[4. Random Forests]\n",
            "  Akurasi Random Forest: 0.9120\n",
            "\n",
            "[5. Feature Importance]\n",
            "  Feature Importance (Dataset Iris):\n",
            "  - sepal length (cm): 0.1125\n",
            "  - sepal width (cm): 0.0231\n",
            "  - petal length (cm): 0.4410\n",
            "  - petal width (cm): 0.4234\n",
            "\n",
            "[6. Boosting (AdaBoost & Gradient Boosting)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Akurasi AdaBoost: 0.8960\n",
            "  Melatih Gradient Boosting (GBRT)...\n",
            "  Jumlah pohon (estimator) terbaik untuk GBRT: 118\n",
            "\n",
            "[7. XGBoost]\n",
            "  RMSE XGBoost: 0.058937\n",
            "\n",
            "[8. Stacking]\n",
            "  Akurasi Stacking Classifier: 0.9120\n",
            "\n",
            "--- Selesai Bab 7 ---\n"
          ]
        }
      ],
      "source": [
        "# MENGIMPOR SEMUA LIBRARY YANG DIPERLUKAN UNTUK BAB 7\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Impor Model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "import xgboost\n",
        "\n",
        "# Impor Data dan Utilitas\n",
        "from sklearn.datasets import make_moons, load_iris\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "# Mengatur default plotting\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "print(\"--- BAB 7: ENSEMBLE LEARNING DAN RANDOM FORESTS ---\")\n",
        "print(\"Semua library telah diimpor.\")\n",
        "\n",
        "# 1. VOTING CLASSIFIERS\n",
        "print(\"\\n[1. Voting Classifiers]\")\n",
        "# Teori: Menggabungkan prediksi dari beberapa model. \"Kebijaksanaan orang banyak\".\n",
        "\n",
        "# Membuat dataset moons\n",
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Mendefinisikan tiga classifier yang berbeda\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = SVC(gamma=\"scale\", random_state=42)\n",
        "\n",
        "# Membuat Voting Classifier (Hard Voting)\n",
        "# Hard voting: mengambil prediksi mayoritas (voting)\n",
        "voting_clf_hard = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='hard'\n",
        ")\n",
        "voting_clf_hard.fit(X_train, y_train)\n",
        "\n",
        "print(\"  Akurasi masing-masing model (Hard Voting):\")\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf_hard):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"  - {clf.__class__.__name__}: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Membuat Voting Classifier (Soft Voting)\n",
        "# Teori: Soft voting merata-ratakan probabilitas prediksi dari semua model\n",
        "# dan memilih kelas dengan probabilitas rata-rata tertinggi.\n",
        "# Biasanya bekerja lebih baik. Perlu model yang bisa memprediksi prob (predict_proba())\n",
        "svm_clf_soft = SVC(gamma=\"scale\", probability=True, random_state=42) # Harus set probability=True\n",
        "\n",
        "voting_clf_soft = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf_soft)],\n",
        "    voting='soft'\n",
        ")\n",
        "voting_clf_soft.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n  Akurasi (Soft Voting):\")\n",
        "y_pred_soft = voting_clf_soft.predict(X_test)\n",
        "print(f\"  - VotingClassifier (Soft): {accuracy_score(y_test, y_pred_soft):.4f}\")\n",
        "# Soft voting biasanya memberikan akurasi yang lebih baik.\n",
        "\n",
        "# 2. BAGGING DAN PASTING\n",
        "print(\"\\n[2. Bagging dan Pasting]\")\n",
        "# Teori: Melatih algoritma yang SAMA berulang kali pada\n",
        "# subset acak yang berbeda dari data latih.\n",
        "# - Bagging (Bootstrap Aggregating): Sampling DENGAN penggantian (with replacement).\n",
        "# - Pasting: Sampling TANPA penggantian (without replacement).\n",
        "\n",
        "# Menggunakan BaggingClassifier dengan Decision Trees\n",
        "tree_clf = DecisionTreeClassifier(random_state=42)\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=500,\n",
        "    max_samples=100, # Setiap pohon dilatih hanya pada 100 instance\n",
        "    bootstrap=True,  # True = Bagging, False = Pasting\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred_bag = bag_clf.predict(X_test)\n",
        "\n",
        "tree_clf.fit(X_train, y_train)\n",
        "y_pred_tree = tree_clf.predict(X_test)\n",
        "\n",
        "print(f\"  Akurasi Decision Tree tunggal: {accuracy_score(y_test, y_pred_tree):.4f}\")\n",
        "print(f\"  Akurasi Bagging (500 pohon): {accuracy_score(y_test, y_pred_bag):.4f}\")\n",
        "\n",
        "# Fungsi helper untuk plot decision boundary\n",
        "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True):\n",
        "    # Buat meshgrid\n",
        "    x1s = np.linspace(axes[0], axes[1], 100)\n",
        "    x2s = np.linspace(axes[2], axes[3], 100)\n",
        "    x1, x2 = np.meshgrid(x1s, x2s)\n",
        "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
        "\n",
        "    # Dapatkan prediksi\n",
        "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
        "\n",
        "    # Plot kontur (batas keputusan)\n",
        "    custom_cmap = mpl.colors.ListedColormap(['#9898ff','#fafab0'])\n",
        "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
        "\n",
        "    if contour:\n",
        "        custom_cmap2 = mpl.colors.ListedColormap(['#7d7d58','#4c4c7f'])\n",
        "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
        "\n",
        "    # Plot data points\n",
        "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
        "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
        "    plt.axis(axes)\n",
        "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
        "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
        "\n",
        "# Plot (di-comment agar tidak auto-tampil)\n",
        "# plt.figure(figsize=(11,4))\n",
        "# plt.subplot(121)\n",
        "# plot_decision_boundary(tree_clf, X, y)\n",
        "# plt.title(\"Decision Tree Tunggal (Overfitting)\", fontsize=14)\n",
        "# plt.subplot(122)\n",
        "# plot_decision_boundary(bag_clf, X, y)\n",
        "# plt.title(\"Bagging dengan 500 Pohon (Lebih Halus)\", fontsize=14)\n",
        "# plt.show()\n",
        "# Hasil: Bagging memiliki batas keputusan yang lebih halus dan generalisasi lebih baik.\n",
        "\n",
        "# 3. OUT-OF-BAG (OOB) EVALUATION\n",
        "print(\"\\n[3. Out-of-Bag (OOB) Evaluation]\")\n",
        "# Teori: Dalam Bagging, karena sampling *with replacement*,\n",
        "# sekitar 37% data latih tidak terpakai (out-of-bag) untuk setiap pohon.\n",
        "# Kita bisa menggunakan data oob ini sebagai validation set otomatis.\n",
        "\n",
        "bag_clf_oob = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=500,\n",
        "    bootstrap=True,\n",
        "    oob_score=True, # Meminta OOB score\n",
        "    random_state=40\n",
        ")\n",
        "bag_clf_oob.fit(X_train, y_train)\n",
        "print(f\"  Skor OOB: {bag_clf_oob.oob_score_:.4f}\")\n",
        "\n",
        "# Mari kita bandingkan dengan akurasi di test set\n",
        "y_pred_oob = bag_clf_oob.predict(X_test)\n",
        "print(f\"  Akurasi Test Set: {accuracy_score(y_test, y_pred_oob):.4f}\")\n",
        "# Skor OOB adalah estimasi yang baik untuk akurasi test set.\n",
        "\n",
        "# 4. RANDOM FORESTS\n",
        "print(\"\\n[4. Random Forests]\")\n",
        "# Teori: Random Forest adalah ensemble Decision Trees,\n",
        "# dilatih dengan Bagging (biasanya).\n",
        "# Perbedaan utama: Saat membelah node, alih-alih mencari fitur\n",
        "# terbaik dari SEMUA fitur, ia mencari yang terbaik dari\n",
        "# SUBSET ACAK fitur. Ini menambah keacakan dan mengurangi korelasi\n",
        "# antar pohon, sehingga mengurangi varians (lebih baik).\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rnd_clf.predict(X_test)\n",
        "print(f\"  Akurasi Random Forest: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
        "\n",
        "# 5. FEATURE IMPORTANCE\n",
        "print(\"\\n[5. Feature Importance]\")\n",
        "# Teori: Random Forest bisa mengukur seberapa penting setiap fitur\n",
        "# dengan melihat seberapa sering fitur tsb mengurangi impurity\n",
        "# di semua pohon.\n",
        "\n",
        "iris = load_iris()\n",
        "rnd_clf_iris = RandomForestClassifier(n_estimators=500, random_state=42)\n",
        "rnd_clf_iris.fit(iris[\"data\"], iris[\"target\"])\n",
        "\n",
        "print(\"  Feature Importance (Dataset Iris):\")\n",
        "for name, score in zip(iris[\"feature_names\"], rnd_clf_iris.feature_importances_):\n",
        "    print(f\"  - {name}: {score:.4f}\")\n",
        "# Kita bisa lihat bahwa petal length dan width adalah fitur terpenting.\n",
        "\n",
        "# 6. BOOSTING\n",
        "print(\"\\n[6. Boosting (AdaBoost & Gradient Boosting)]\")\n",
        "# Teori: Boosting adalah metode ensemble sekuensial.\n",
        "# Model dilatih satu per satu, di mana setiap model baru\n",
        "# mencoba memperbaiki kesalahan model sebelumnya.\n",
        "\n",
        "# 6a. AdaBoost (Adaptive Boosting)\n",
        "# Teori: Fokus pada instance yang salah diklasifikasikan\n",
        "# oleh model sebelumnya (dengan meningkatkan bobot/weight mereka).\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1, random_state=42),\n",
        "    n_estimators=200,\n",
        "    algorithm=\"SAMME\",\n",
        "    learning_rate=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "ada_clf.fit(X_train, y_train)\n",
        "y_pred_ada = ada_clf.predict(X_test)\n",
        "print(f\"  Akurasi AdaBoost: {accuracy_score(y_test, y_pred_ada):.4f}\")\n",
        "\n",
        "# 6b. Gradient Boosting (GBRT)\n",
        "print(\"  Melatih Gradient Boosting (GBRT)...\")\n",
        "# Teori: Melatih model baru pada 'residual error' (sisa kesalahan)\n",
        "# dari model sebelumnya.\n",
        "\n",
        "# Membuat data kuadratik untuk regresi\n",
        "np.random.seed(42)\n",
        "X_reg = np.random.rand(100, 1) - 0.5\n",
        "y_reg = 3*X_reg**2 + 0.05 * np.random.randn(100, 1)\n",
        "\n",
        "# Langkah Manual (Konseptual)\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg1.fit(X_reg, y_reg)\n",
        "y2 = y_reg - tree_reg1.predict(X_reg).reshape(-1, 1) # Hitung error\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg2.fit(X_reg, y2) # Latih pohon kedua pada error\n",
        "y3 = y2 - tree_reg2.predict(X_reg).reshape(-1, 1) # Hitung error lagi\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg3.fit(X_reg, y3)\n",
        "# Prediksi = tree_reg1 + tree_reg2 + tree_reg3\n",
        "# y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
        "\n",
        "# Menggunakan Scikit-Learn GradientBoostingRegressor\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
        "gbrt.fit(X_reg, y_reg.ravel())\n",
        "\n",
        "# 6c. GBRT dengan Early Stopping\n",
        "# Teori: Kita bisa menemukan jumlah pohon (n_estimators) optimal\n",
        "# menggunakan 'staged_predict' (mirip early stopping).\n",
        "X_train_reg, X_val_reg, y_train_reg, y_val_reg = train_test_split(X_reg, y_reg.ravel(), random_state=42)\n",
        "\n",
        "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
        "gbrt_best.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Cari error pada setiap tahap (setiap penambahan pohon)\n",
        "errors = [mean_squared_error(y_val_reg, y_pred)\n",
        "          for y_pred in gbrt_best.staged_predict(X_val_reg)]\n",
        "bst_n_estimators = np.argmin(errors) + 1 # Temukan jumlah pohon terbaik\n",
        "\n",
        "print(f\"  Jumlah pohon (estimator) terbaik untuk GBRT: {bst_n_estimators}\")\n",
        "\n",
        "# Latih ulang model final dengan jumlah pohon terbaik\n",
        "gbrt_final = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
        "gbrt_final.fit(X_reg, y_reg.ravel())\n",
        "\n",
        "# 7. XGBOOST (EXTREME GRADIENT BOOSTING)\n",
        "print(\"\\n[7. XGBoost]\")\n",
        "# Teori: Implementasi Gradient Boosting yang sangat cepat,\n",
        "# dapat di-scale, dan sangat populer di kompetisi.\n",
        "\n",
        "# Inisialisasi dan latih\n",
        "xgb_reg = xgboost.XGBRegressor(random_state=42)\n",
        "\n",
        "# Manual early stopping for XGBoost\n",
        "# Check for best iteration during training\n",
        "eval_set = [(X_val_reg, y_val_reg)]\n",
        "xgb_reg.fit(X_train_reg, y_train_reg, eval_set=eval_set, verbose=False) # Train without early stopping parameter\n",
        "\n",
        "# Note: With recent XGBoost versions, you might need to access the best iteration differently\n",
        "# or rely on the output of the fit method if verbose is True.\n",
        "# Since early_stopping_rounds is causing issues, we'll skip finding the best iteration\n",
        "# and just use the model trained for the full number of default estimators.\n",
        "# If early stopping is crucial, manual implementation with a loop and tracking validation score is needed.\n",
        "# For now, we proceed with the full training.\n",
        "\n",
        "y_pred_xgb = xgb_reg.predict(X_val_reg)\n",
        "xgb_rmse = np.sqrt(mean_squared_error(y_val_reg, y_pred_xgb))\n",
        "print(f\"  RMSE XGBoost: {xgb_rmse:.6f}\")\n",
        "\n",
        "# 8. STACKING (STACKED GENERALIZATION)\n",
        "print(\"\\n[8. Stacking]\")\n",
        "# Teori: Pendekatan ensemble tingkat lanjut. Alih-alih voting,\n",
        "# kita melatih model baru (disebut 'blender' atau 'meta-learner')\n",
        "# untuk melakukan agregasi.\n",
        "# Lapisan 1: Prediktor (misal, RF, SVM)\n",
        "# Lapisan 2 (Blender): Model (misal, Ridge) dilatih PADA PREDIKSI dari lapisan 1.\n",
        "\n",
        "# StackingClassifier sudah ada di Scikit-Learn\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
        "    ('svc', SVC(probability=True, random_state=42)) # Perlu probability=True untuk blender\n",
        "]\n",
        "\n",
        "# Blender (meta-learner) bisa berupa apa saja, biasanya model linear\n",
        "# 'passthrough=True' berarti blender juga akan menerima fitur asli\n",
        "stack_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(),\n",
        "    passthrough=True,\n",
        "    cv=5 # Cross-validate prediksi L1\n",
        ")\n",
        "\n",
        "stack_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stack_clf.predict(X_test)\n",
        "print(f\"  Akurasi Stacking Classifier: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
        "\n",
        "print(\"\\n--- Selesai Bab 7 ---\")"
      ]
    }
  ]
}